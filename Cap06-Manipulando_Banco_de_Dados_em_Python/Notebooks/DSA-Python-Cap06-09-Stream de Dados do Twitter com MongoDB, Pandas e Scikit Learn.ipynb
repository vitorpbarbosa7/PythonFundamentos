{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Data Science Academy - Python Fundamentos - Capítulo 6</font>\n",
    "\n",
    "## Download: http://github.com/dsacademybr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream de Dados do Twitter com MongoDB, Pandas e Scikit Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparando a Conexão com o Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tweepy\n",
      "  Downloading https://files.pythonhosted.org/packages/36/1b/2bd38043d22ade352fc3d3902cf30ce0e2f4bf285be3b304a2782a767aec/tweepy-3.8.0-py2.py3-none-any.whl\n",
      "Collecting requests-oauthlib>=0.7.0 (from tweepy)\n",
      "  Downloading https://files.pythonhosted.org/packages/a3/12/b92740d845ab62ea4edf04d2f4164d82532b5a0b03836d4d4e71c6f3d379/requests_oauthlib-1.3.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: requests>=2.11.1 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from tweepy) (2.19.1)\n",
      "Requirement already satisfied: PySocks>=1.5.7 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from tweepy) (1.6.8)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from tweepy) (1.11.0)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->tweepy)\n",
      "  Downloading https://files.pythonhosted.org/packages/05/57/ce2e7a8fa7c0afb54a0581b14a65b56e62b5759dbc98e80627142b8a3704/oauthlib-3.1.0-py2.py3-none-any.whl (147kB)\n",
      "Requirement already satisfied: urllib3<1.24,>=1.21.1 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from requests>=2.11.1->tweepy) (1.23)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from requests>=2.11.1->tweepy) (3.0.4)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from requests>=2.11.1->tweepy) (2.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from requests>=2.11.1->tweepy) (2018.8.24)\n",
      "Installing collected packages: oauthlib, requests-oauthlib, tweepy\n",
      "Successfully installed oauthlib-3.1.0 requests-oauthlib-1.3.0 tweepy-3.8.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "orange3 3.24.1 requires keyrings.alt, which is not installed.\n",
      "orange3 3.24.1 requires python-louvain>=0.13, which is not installed.\n",
      "orange3 3.24.1 requires serverfiles, which is not installed.\n",
      "twisted 18.7.0 requires PyHamcrest>=1.9.0, which is not installed.\n",
      "orange3 3.24.1 has requirement numpy>=1.16.0, but you'll have numpy 1.15.1 which is incompatible.\n",
      "orange-canvas-core 0.1.11 has requirement pip>=18.0, but you'll have pip 10.0.1 which is incompatible.\n",
      "You are using pip version 10.0.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# Instala o pacote tweepy\n",
    "!pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando os módulos Tweepy, Datetime e Json\n",
    "from tweepy.streaming import StreamListener #Ouvir pelo twitter\n",
    "from tweepy import OAuthHandler #Autenticação pelo twitter\n",
    "from tweepy import Stream #Classe de tratamento de streaming do pacote tweepy\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veja no manual em pdf como criar sua API no Twitter e configure as suas chaves abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adicione aqui sua Consumer Key\n",
    "consumer_key = \"ts1VSG249kwQZt3G4gfOYlT9w\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adicione aqui sua Consumer Secret \n",
    "consumer_secret = \"c23ogyLe3kBCMAQ49IST9DEPCdaIirvCkP0WMBh18z4Uq1thzl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adicione aqui seu Access Token\n",
    "access_token = \"1174700278043611141-S8PkoBl4bEtJyKetkLbtuPt9LU6N93\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adicione aqui seu Access Token Secret\n",
    "access_token_secret = \"TBmStUq5HrgXUcoH4gYbdVdE0YH9g5WYZLSBLZCIbzrkT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando as chaves de autenticação\n",
    "#Criação do objeto de autenticação a partir da classe importada de autenticação \n",
    "auth = OAuthHandler(consumer_key, consumer_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth.set_access_token(access_token, access_token_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando uma classe para capturar os stream de dados do Twitter e \n",
    "# armazenar no MongoDB\n",
    "class MyListener(StreamListener):   \n",
    "    \n",
    "    def __init__(self, time_limit = 60):\n",
    "        self.start_time = time.time()\n",
    "        self.limit = time_limit\n",
    "        self.saveFile = open('streamings.json', 'a')\n",
    "        super(StreamListener, self).__init__()\n",
    "        \n",
    "    def on_data(self, dados):\n",
    "        if (time.time() - self.start_time) < self.limit:\n",
    "#             abcd. = json.loads(dados) #Conversão para o formato json para gravar no \n",
    "#             #mongodb\n",
    "            streamings = json.loads(dados)\n",
    "            id_ = streamings[\"id\"]\n",
    "            created_at = streamings[\"created_at\"]\n",
    "            id_str = streamings[\"id_str\"]\n",
    "            text = streamings[\"text\"]\n",
    "            obj = {\"id\":id_,\"created_at\":created_at,\"id_str\":id_str,\"text\":text,}\n",
    "            tweetind = col.insert_one(obj).inserted_id\n",
    "            print (obj)\n",
    "            self.saveFile.write(dados)\n",
    "            self.saveFile.write('\\n')\n",
    "            return True\n",
    "        else:\n",
    "            self.saveFile.close()\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando o objeto mylistener\n",
    "mylistener = MyListener()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando o objeto mystream\n",
    "#Isntanciando o objeto mystream\n",
    "mystream = Stream(auth, listener = MyListener(time_limit = 20))\n",
    "#mystream = Stream(minhas credenciais, meu objeto para ouvir os tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparando a Conexão com o MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pymongo'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-ee265e7fdbee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Importando do PyMongo o módulo MongoClient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpymongo\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMongoClient\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pymongo'"
     ]
    }
   ],
   "source": [
    "# Importando do PyMongo o módulo MongoClient\n",
    "from pymongo import MongoClient   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando a conexão ao MongoDB\n",
    "client = MongoClient('localhost', 27017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando o banco de dados twitterdb\n",
    "db = client.twitterdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando a collection \"col\"\n",
    "col = db.tweets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando uma lista de palavras chave para buscar nos Tweets\n",
    "keywords = ['Data Science']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coletando os Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iniciando o filtro e gravando os tweets no MongoDB\n",
    "# mystream.tweepy.Stream(auth = api.auth, listener = StreamListener(time_limit=20))\n",
    "mystream.filter(track=keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --> Pressione o botão Stop na barra de ferramentas para encerrar a captura dos Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consultando os Dados no MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystream.disconnect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': ObjectId('5de27752075f4da22e0a62ab'),\n",
       " 'id': 1200778024431562759,\n",
       " 'created_at': 'Sat Nov 30 14:06:05 +0000 2019',\n",
       " 'id_str': '1200778024431562759',\n",
       " 'text': \"RT @Neenar568: OK...it's a toss up~~I'm dying!!!! 🤣😂😂🤣 https://t.co/CJ02UHvxVY\"}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verificando um documento no collection\n",
    "col.find_one()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise de Dados com Pandas e Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando um dataset com dados retornados do MongoDB\n",
    "#Retornar os dados para cada item na coleção (Cada item é um documento)\n",
    "dataset = [{\"created_at\": item[\"created_at\"], \"text\": item[\"text\"],} for item in col.find()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando o módulo Pandas para trabalhar com datasets em Python\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando um dataframe a partir do dataset \n",
    "df = pd.DataFrame(dataset) #Criação de um dataframe (objetos com formato de colunas e linhas) a partir do banco de dados dataset presente no mongodb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sat Nov 30 14:06:05 +0000 2019</td>\n",
       "      <td>RT @Neenar568: OK...it's a toss up~~I'm dying!...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sat Nov 30 14:06:05 +0000 2019</td>\n",
       "      <td>@AdeBARRcode @ravenhaired_sub In essence it is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sat Nov 30 14:06:05 +0000 2019</td>\n",
       "      <td>RT @jessphillips: Answer the question you spin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sat Nov 30 14:06:05 +0000 2019</td>\n",
       "      <td>Pahiram netflix acc HAHA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sat Nov 30 14:06:06 +0000 2019</td>\n",
       "      <td>\"our Netflix\" sounds so domestic can you.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sat Nov 30 14:06:06 +0000 2019</td>\n",
       "      <td>SAME! Definitely nearing triple digits #renewa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sat Nov 30 14:06:06 +0000 2019</td>\n",
       "      <td>My fucking 123movuee geekin , lemme hold someb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sat Nov 30 14:06:07 +0000 2019</td>\n",
       "      <td>RT @travelkaan: น่ารักมาก ที่พักที่เวียดนาม ถู...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sat Nov 30 14:06:07 +0000 2019</td>\n",
       "      <td>Ultimato😱</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sat Nov 30 14:06:07 +0000 2019</td>\n",
       "      <td>@Mr_Barnett10 8 hours of netflix...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Sat Nov 30 14:06:08 +0000 2019</td>\n",
       "      <td>RT @RBReich: What do these corporations have i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Sat Nov 30 14:06:08 +0000 2019</td>\n",
       "      <td>RT @gabalexa: can someone explain why Netflix ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Sat Nov 30 14:06:08 +0000 2019</td>\n",
       "      <td>RT @vinyl_tackey: 『カンフー・ヨガ』はNetflixで配信か〜と思っていた...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Sat Nov 30 14:06:08 +0000 2019</td>\n",
       "      <td>RT @gemma_st_cloud: Wouldn't it be wonderful t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Sat Nov 30 14:06:09 +0000 2019</td>\n",
       "      <td>RT @DanielJJonesUS: Re-Print of the Senate “To...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Sat Nov 30 14:06:10 +0000 2019</td>\n",
       "      <td>RT @Komod0o: 9000 abonnés 😝 ça m’a donné envie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Sat Nov 30 14:06:10 +0000 2019</td>\n",
       "      <td>RT @fruitsvsten: And he had that really proud ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Sat Nov 30 14:06:10 +0000 2019</td>\n",
       "      <td>RT @shawntvamps: não desmerecendo outras série...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Sat Nov 30 14:06:10 +0000 2019</td>\n",
       "      <td>This is such an amaizing show that must contin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Sat Nov 30 14:06:11 +0000 2019</td>\n",
       "      <td>RT @pppisut: Met A Space Pod สาขาอโศก\\nอยู่ดีๆ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Sat Nov 30 14:06:11 +0000 2019</td>\n",
       "      <td>12 years today I was producing my uni professo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Sat Nov 30 14:06:11 +0000 2019</td>\n",
       "      <td>RT @paulwaugh: Now @BorisJohnson really is in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Sat Nov 30 14:06:11 +0000 2019</td>\n",
       "      <td>RT @yajpeg: actually shocked by the backlash t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Sat Nov 30 14:06:11 +0000 2019</td>\n",
       "      <td>RT @BASE_JUALAN: sell JUAL APP PREMIUM, 100% L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Sat Nov 30 14:06:11 +0000 2019</td>\n",
       "      <td>RT @iispace64: So disney plus doesn’t have\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Sat Nov 30 14:06:13 +0000 2019</td>\n",
       "      <td>@lxrelaaa @iMatxlda Now the real question is ....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Sat Nov 30 14:06:13 +0000 2019</td>\n",
       "      <td>@leonardotupi Quando chegar o Disney Plus aqui...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Sat Nov 30 14:06:14 +0000 2019</td>\n",
       "      <td>RT @alxrgs: ENFIN !!!! https://t.co/HC9OGuyKBO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Sat Nov 30 14:06:14 +0000 2019</td>\n",
       "      <td>i wanna finish these reqs na so i can swim, ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Sat Nov 30 14:06:14 +0000 2019</td>\n",
       "      <td>RT @RBReich: What do these corporations have i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Sat Nov 30 14:06:16 +0000 2019</td>\n",
       "      <td>RT @travelkaan: น่ารักมาก ที่พักที่เวียดนาม ถู...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Sat Nov 30 14:06:16 +0000 2019</td>\n",
       "      <td>RT @TheNightCircusx: I cry whenever I see anyt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Sat Nov 30 14:06:16 +0000 2019</td>\n",
       "      <td>870K HQ Combo (Spotify,Deezer,Crunchy,Netflix,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Sat Nov 30 14:06:16 +0000 2019</td>\n",
       "      <td>หาร Viu Premium \\nราคาถูกมากกกก แม่ค้าตอบเร็วย...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Sat Nov 30 14:06:17 +0000 2019</td>\n",
       "      <td>Tá quando q saí o exo no knowing brothers na N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Sat Nov 30 14:06:17 +0000 2019</td>\n",
       "      <td>RT @fruitsvsten: And he had that really proud ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Sat Nov 30 14:06:17 +0000 2019</td>\n",
       "      <td>RT @plausible4me: And I paid a ton of taxes.  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Sat Nov 30 14:06:18 +0000 2019</td>\n",
       "      <td>New Episodes of The Proud Family Are Headed to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Sat Nov 30 14:06:18 +0000 2019</td>\n",
       "      <td>RT @CadenaDeSeguido: NOS SEGUIMOS TODOS \\n\\n🃏R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Sat Nov 30 14:06:19 +0000 2019</td>\n",
       "      <td>RT @Channel4News: Sir David Attenborough says ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Sat Nov 30 14:06:19 +0000 2019</td>\n",
       "      <td>@Netflixhelps @33Arduc @e73FxFSDzOGwsi7 @netfl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Sat Nov 30 14:06:19 +0000 2019</td>\n",
       "      <td>RT @IGN: Prime Stranding. https://t.co/bDto2es1fa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Sat Nov 30 14:06:20 +0000 2019</td>\n",
       "      <td>RT @GossipRoomOff: Netflix développe actuellem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Sat Nov 30 14:06:20 +0000 2019</td>\n",
       "      <td>Porra vai demorar pra caralho pra esse episódi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Sat Nov 30 14:06:20 +0000 2019</td>\n",
       "      <td>RT @dunerfors: I created a viewing guide for e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Sat Nov 30 14:06:20 +0000 2019</td>\n",
       "      <td>RT @MovNaranja3: Sabía que transportadores no ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Sat Nov 30 14:06:21 +0000 2019</td>\n",
       "      <td>RT @shirbertluv: WE ARE DOING THIS FOR HER #re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Sat Nov 30 14:06:22 +0000 2019</td>\n",
       "      <td>RT @EMahmoudSoliman: تحذير هام\\nرسائل بريد إلك...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Sat Nov 30 14:06:22 +0000 2019</td>\n",
       "      <td>@Belleme_ ทางนี้กำลังหาnetflixดูไปเรื่อยเลยครั...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Sat Nov 30 14:06:22 +0000 2019</td>\n",
       "      <td>RT @_sandraribeiro_: Combinem isto comigo, jur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Sat Nov 30 14:06:22 +0000 2019</td>\n",
       "      <td>RT @GossipRoomOff: Netflix développe actuellem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Sat Nov 30 14:06:22 +0000 2019</td>\n",
       "      <td>RT @fatboy22642: Lol..Ok I’m dying over here!!...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Sat Nov 30 14:06:22 +0000 2019</td>\n",
       "      <td>RT @CCNews__: Sabía que pasar tanto tiempo vie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Sat Nov 30 14:06:22 +0000 2019</td>\n",
       "      <td>RT @anneskindred: still lowkey think netflix w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Sat Nov 30 14:06:22 +0000 2019</td>\n",
       "      <td>RT @nicktolhurst: Boris Johnson uses his condo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Sat Nov 30 14:06:22 +0000 2019</td>\n",
       "      <td>eu não acredito que revenge entrou na globo pl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Sat Nov 30 14:06:23 +0000 2019</td>\n",
       "      <td>RT @Desiiah: Bon, j’écris la liste des films q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Sat Nov 30 14:06:23 +0000 2019</td>\n",
       "      <td>13 Reasons Why: Season 3 | Official Trailer | ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Sat Nov 30 14:06:23 +0000 2019</td>\n",
       "      <td>RT @ColoraSeller: GO INSTALER PI NETWORK ET ME...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>Sat Nov 30 14:06:23 +0000 2019</td>\n",
       "      <td>お家のテレビ確認したらHDMI？の穴空いてたからHDMIケーブル？とあの機械？買ったらテレビ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        created_at  \\\n",
       "0   Sat Nov 30 14:06:05 +0000 2019   \n",
       "1   Sat Nov 30 14:06:05 +0000 2019   \n",
       "2   Sat Nov 30 14:06:05 +0000 2019   \n",
       "3   Sat Nov 30 14:06:05 +0000 2019   \n",
       "4   Sat Nov 30 14:06:06 +0000 2019   \n",
       "5   Sat Nov 30 14:06:06 +0000 2019   \n",
       "6   Sat Nov 30 14:06:06 +0000 2019   \n",
       "7   Sat Nov 30 14:06:07 +0000 2019   \n",
       "8   Sat Nov 30 14:06:07 +0000 2019   \n",
       "9   Sat Nov 30 14:06:07 +0000 2019   \n",
       "10  Sat Nov 30 14:06:08 +0000 2019   \n",
       "11  Sat Nov 30 14:06:08 +0000 2019   \n",
       "12  Sat Nov 30 14:06:08 +0000 2019   \n",
       "13  Sat Nov 30 14:06:08 +0000 2019   \n",
       "14  Sat Nov 30 14:06:09 +0000 2019   \n",
       "15  Sat Nov 30 14:06:10 +0000 2019   \n",
       "16  Sat Nov 30 14:06:10 +0000 2019   \n",
       "17  Sat Nov 30 14:06:10 +0000 2019   \n",
       "18  Sat Nov 30 14:06:10 +0000 2019   \n",
       "19  Sat Nov 30 14:06:11 +0000 2019   \n",
       "20  Sat Nov 30 14:06:11 +0000 2019   \n",
       "21  Sat Nov 30 14:06:11 +0000 2019   \n",
       "22  Sat Nov 30 14:06:11 +0000 2019   \n",
       "23  Sat Nov 30 14:06:11 +0000 2019   \n",
       "24  Sat Nov 30 14:06:11 +0000 2019   \n",
       "25  Sat Nov 30 14:06:13 +0000 2019   \n",
       "26  Sat Nov 30 14:06:13 +0000 2019   \n",
       "27  Sat Nov 30 14:06:14 +0000 2019   \n",
       "28  Sat Nov 30 14:06:14 +0000 2019   \n",
       "29  Sat Nov 30 14:06:14 +0000 2019   \n",
       "..                             ...   \n",
       "35  Sat Nov 30 14:06:16 +0000 2019   \n",
       "36  Sat Nov 30 14:06:16 +0000 2019   \n",
       "37  Sat Nov 30 14:06:16 +0000 2019   \n",
       "38  Sat Nov 30 14:06:16 +0000 2019   \n",
       "39  Sat Nov 30 14:06:17 +0000 2019   \n",
       "40  Sat Nov 30 14:06:17 +0000 2019   \n",
       "41  Sat Nov 30 14:06:17 +0000 2019   \n",
       "42  Sat Nov 30 14:06:18 +0000 2019   \n",
       "43  Sat Nov 30 14:06:18 +0000 2019   \n",
       "44  Sat Nov 30 14:06:19 +0000 2019   \n",
       "45  Sat Nov 30 14:06:19 +0000 2019   \n",
       "46  Sat Nov 30 14:06:19 +0000 2019   \n",
       "47  Sat Nov 30 14:06:20 +0000 2019   \n",
       "48  Sat Nov 30 14:06:20 +0000 2019   \n",
       "49  Sat Nov 30 14:06:20 +0000 2019   \n",
       "50  Sat Nov 30 14:06:20 +0000 2019   \n",
       "51  Sat Nov 30 14:06:21 +0000 2019   \n",
       "52  Sat Nov 30 14:06:22 +0000 2019   \n",
       "53  Sat Nov 30 14:06:22 +0000 2019   \n",
       "54  Sat Nov 30 14:06:22 +0000 2019   \n",
       "55  Sat Nov 30 14:06:22 +0000 2019   \n",
       "56  Sat Nov 30 14:06:22 +0000 2019   \n",
       "57  Sat Nov 30 14:06:22 +0000 2019   \n",
       "58  Sat Nov 30 14:06:22 +0000 2019   \n",
       "59  Sat Nov 30 14:06:22 +0000 2019   \n",
       "60  Sat Nov 30 14:06:22 +0000 2019   \n",
       "61  Sat Nov 30 14:06:23 +0000 2019   \n",
       "62  Sat Nov 30 14:06:23 +0000 2019   \n",
       "63  Sat Nov 30 14:06:23 +0000 2019   \n",
       "64  Sat Nov 30 14:06:23 +0000 2019   \n",
       "\n",
       "                                                 text  \n",
       "0   RT @Neenar568: OK...it's a toss up~~I'm dying!...  \n",
       "1   @AdeBARRcode @ravenhaired_sub In essence it is...  \n",
       "2   RT @jessphillips: Answer the question you spin...  \n",
       "3                            Pahiram netflix acc HAHA  \n",
       "4           \"our Netflix\" sounds so domestic can you.  \n",
       "5   SAME! Definitely nearing triple digits #renewa...  \n",
       "6   My fucking 123movuee geekin , lemme hold someb...  \n",
       "7   RT @travelkaan: น่ารักมาก ที่พักที่เวียดนาม ถู...  \n",
       "8                                           Ultimato😱  \n",
       "9                 @Mr_Barnett10 8 hours of netflix...  \n",
       "10  RT @RBReich: What do these corporations have i...  \n",
       "11  RT @gabalexa: can someone explain why Netflix ...  \n",
       "12  RT @vinyl_tackey: 『カンフー・ヨガ』はNetflixで配信か〜と思っていた...  \n",
       "13  RT @gemma_st_cloud: Wouldn't it be wonderful t...  \n",
       "14  RT @DanielJJonesUS: Re-Print of the Senate “To...  \n",
       "15  RT @Komod0o: 9000 abonnés 😝 ça m’a donné envie...  \n",
       "16  RT @fruitsvsten: And he had that really proud ...  \n",
       "17  RT @shawntvamps: não desmerecendo outras série...  \n",
       "18  This is such an amaizing show that must contin...  \n",
       "19  RT @pppisut: Met A Space Pod สาขาอโศก\\nอยู่ดีๆ...  \n",
       "20  12 years today I was producing my uni professo...  \n",
       "21  RT @paulwaugh: Now @BorisJohnson really is in ...  \n",
       "22  RT @yajpeg: actually shocked by the backlash t...  \n",
       "23  RT @BASE_JUALAN: sell JUAL APP PREMIUM, 100% L...  \n",
       "24  RT @iispace64: So disney plus doesn’t have\\n\\n...  \n",
       "25  @lxrelaaa @iMatxlda Now the real question is ....  \n",
       "26  @leonardotupi Quando chegar o Disney Plus aqui...  \n",
       "27     RT @alxrgs: ENFIN !!!! https://t.co/HC9OGuyKBO  \n",
       "28  i wanna finish these reqs na so i can swim, ne...  \n",
       "29  RT @RBReich: What do these corporations have i...  \n",
       "..                                                ...  \n",
       "35  RT @travelkaan: น่ารักมาก ที่พักที่เวียดนาม ถู...  \n",
       "36  RT @TheNightCircusx: I cry whenever I see anyt...  \n",
       "37  870K HQ Combo (Spotify,Deezer,Crunchy,Netflix,...  \n",
       "38  หาร Viu Premium \\nราคาถูกมากกกก แม่ค้าตอบเร็วย...  \n",
       "39  Tá quando q saí o exo no knowing brothers na N...  \n",
       "40  RT @fruitsvsten: And he had that really proud ...  \n",
       "41  RT @plausible4me: And I paid a ton of taxes.  ...  \n",
       "42  New Episodes of The Proud Family Are Headed to...  \n",
       "43  RT @CadenaDeSeguido: NOS SEGUIMOS TODOS \\n\\n🃏R...  \n",
       "44  RT @Channel4News: Sir David Attenborough says ...  \n",
       "45  @Netflixhelps @33Arduc @e73FxFSDzOGwsi7 @netfl...  \n",
       "46  RT @IGN: Prime Stranding. https://t.co/bDto2es1fa  \n",
       "47  RT @GossipRoomOff: Netflix développe actuellem...  \n",
       "48  Porra vai demorar pra caralho pra esse episódi...  \n",
       "49  RT @dunerfors: I created a viewing guide for e...  \n",
       "50  RT @MovNaranja3: Sabía que transportadores no ...  \n",
       "51  RT @shirbertluv: WE ARE DOING THIS FOR HER #re...  \n",
       "52  RT @EMahmoudSoliman: تحذير هام\\nرسائل بريد إلك...  \n",
       "53  @Belleme_ ทางนี้กำลังหาnetflixดูไปเรื่อยเลยครั...  \n",
       "54  RT @_sandraribeiro_: Combinem isto comigo, jur...  \n",
       "55  RT @GossipRoomOff: Netflix développe actuellem...  \n",
       "56  RT @fatboy22642: Lol..Ok I’m dying over here!!...  \n",
       "57  RT @CCNews__: Sabía que pasar tanto tiempo vie...  \n",
       "58  RT @anneskindred: still lowkey think netflix w...  \n",
       "59  RT @nicktolhurst: Boris Johnson uses his condo...  \n",
       "60  eu não acredito que revenge entrou na globo pl...  \n",
       "61  RT @Desiiah: Bon, j’écris la liste des films q...  \n",
       "62  13 Reasons Why: Season 3 | Official Trailer | ...  \n",
       "63  RT @ColoraSeller: GO INSTALER PI NETWORK ET ME...  \n",
       "64  お家のテレビ確認したらHDMI？の穴空いてたからHDMIケーブル？とあの機械？買ったらテレビ...  \n",
       "\n",
       "[65 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imprimindo o dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando o módulo Scikit Learn\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usando o método CountVectorizer para criar uma matriz de documentos\n",
    "cv = CountVectorizer()\n",
    "count_matrix = cv.fit_transform(df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rt</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>netflix</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>co</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>and</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>is</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>to</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>it</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>we</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>of</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>he</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>de</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>in</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>for</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>que</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>na</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>plus</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>my</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>this</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>jual</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>up</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>so</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>are</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>on</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>now</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>like</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>northwoodenter</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>cbc</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>proud</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>les</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>me</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>what</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>just</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>his</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>renewannewithane</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>prime</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>can</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>un</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>no</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>that</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>was</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>long</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>et</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>you</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>disney</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>do</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>have</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>pra</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>amazon</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                word  count\n",
       "0                 rt     40\n",
       "1            netflix     29\n",
       "2              https     23\n",
       "3                 co     22\n",
       "4                the     17\n",
       "5                and     11\n",
       "6                 is      9\n",
       "7                 to      9\n",
       "8                 it      8\n",
       "9                 we      7\n",
       "10                of      7\n",
       "11                he      6\n",
       "12                de      6\n",
       "13                in      6\n",
       "14               for      6\n",
       "15               que      5\n",
       "16                na      5\n",
       "17              plus      5\n",
       "18                my      5\n",
       "19              this      5\n",
       "20              jual      5\n",
       "21                up      4\n",
       "22                so      4\n",
       "23               are      4\n",
       "24                on      4\n",
       "25               now      4\n",
       "26              like      4\n",
       "27    northwoodenter      4\n",
       "28               cbc      4\n",
       "29             proud      4\n",
       "30               les      4\n",
       "31                me      4\n",
       "32              what      4\n",
       "33              just      4\n",
       "34               his      4\n",
       "35  renewannewithane      4\n",
       "36             prime      4\n",
       "37               can      3\n",
       "38                un      3\n",
       "39                no      3\n",
       "40              that      3\n",
       "41               was      3\n",
       "42              long      3\n",
       "43                et      3\n",
       "44               you      3\n",
       "45            disney      3\n",
       "46                do      3\n",
       "47              have      3\n",
       "48               pra      3\n",
       "49            amazon      3"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Contando o número de ocorrências das principais palavras em nosso dataset\n",
    "word_count = pd.DataFrame(cv.get_feature_names(), columns=[\"word\"])\n",
    "word_count[\"count\"] = count_matrix.sum(axis=0).tolist()[0]\n",
    "word_count = word_count.sort_values(\"count\", ascending=False).reset_index(drop=True)\n",
    "word_count[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obrigado - Data Science Academy - <a href=\"http://facebook.com/dsacademybr\">facebook.com/dsacademybr</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
